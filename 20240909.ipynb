{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting azure-ai-textanalytics==5.2.0\n",
      "  Downloading azure_ai_textanalytics-5.2.0-py3-none-any.whl.metadata (69 kB)\n",
      "Collecting azure-core<2.0.0,>=1.24.0 (from azure-ai-textanalytics==5.2.0)\n",
      "  Downloading azure_core-1.30.2-py3-none-any.whl.metadata (37 kB)\n",
      "Collecting msrest>=0.7.0 (from azure-ai-textanalytics==5.2.0)\n",
      "  Downloading msrest-0.7.1-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting azure-common~=1.1 (from azure-ai-textanalytics==5.2.0)\n",
      "  Downloading azure_common-1.1.28-py2.py3-none-any.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.0.1 in c:\\users\\user\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from azure-ai-textanalytics==5.2.0) (4.12.2)\n",
      "Requirement already satisfied: requests>=2.21.0 in c:\\users\\user\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from azure-core<2.0.0,>=1.24.0->azure-ai-textanalytics==5.2.0) (2.32.3)\n",
      "Requirement already satisfied: six>=1.11.0 in c:\\users\\user\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from azure-core<2.0.0,>=1.24.0->azure-ai-textanalytics==5.2.0) (1.16.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from msrest>=0.7.0->azure-ai-textanalytics==5.2.0) (2024.7.4)\n",
      "Collecting isodate>=0.6.0 (from msrest>=0.7.0->azure-ai-textanalytics==5.2.0)\n",
      "  Downloading isodate-0.6.1-py2.py3-none-any.whl.metadata (9.6 kB)\n",
      "Collecting requests-oauthlib>=0.5.0 (from msrest>=0.7.0->azure-ai-textanalytics==5.2.0)\n",
      "  Using cached requests_oauthlib-2.0.0-py2.py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\user\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests>=2.21.0->azure-core<2.0.0,>=1.24.0->azure-ai-textanalytics==5.2.0) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\user\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests>=2.21.0->azure-core<2.0.0,>=1.24.0->azure-ai-textanalytics==5.2.0) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\user\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests>=2.21.0->azure-core<2.0.0,>=1.24.0->azure-ai-textanalytics==5.2.0) (2.2.2)\n",
      "Collecting oauthlib>=3.0.0 (from requests-oauthlib>=0.5.0->msrest>=0.7.0->azure-ai-textanalytics==5.2.0)\n",
      "  Using cached oauthlib-3.2.2-py3-none-any.whl.metadata (7.5 kB)\n",
      "Downloading azure_ai_textanalytics-5.2.0-py3-none-any.whl (239 kB)\n",
      "Downloading azure_common-1.1.28-py2.py3-none-any.whl (14 kB)\n",
      "Downloading azure_core-1.30.2-py3-none-any.whl (194 kB)\n",
      "Downloading msrest-0.7.1-py3-none-any.whl (85 kB)\n",
      "Downloading isodate-0.6.1-py2.py3-none-any.whl (41 kB)\n",
      "Using cached requests_oauthlib-2.0.0-py2.py3-none-any.whl (24 kB)\n",
      "Using cached oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "Installing collected packages: azure-common, oauthlib, isodate, requests-oauthlib, azure-core, msrest, azure-ai-textanalytics\n",
      "Successfully installed azure-ai-textanalytics-5.2.0 azure-common-1.1.28 azure-core-1.30.2 isodate-0.6.1 msrest-0.7.1 oauthlib-3.2.2 requests-oauthlib-2.0.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install azure-ai-textanalytics==5.2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Named Entities:\n",
      "\n",
      "\tText: \t Microsoft \tCategory: \t Organization \tSubCategory: \t None \n",
      "\tConfidence Score: \t 0.99 \tLength: \t 9 \tOffset: \t 0 \n",
      "\n",
      "\tText: \t Bill Gates \tCategory: \t Person \tSubCategory: \t None \n",
      "\tConfidence Score: \t 1.0 \tLength: \t 10 \tOffset: \t 25 \n",
      "\n",
      "\tText: \t Paul Allen \tCategory: \t Person \tSubCategory: \t None \n",
      "\tConfidence Score: \t 1.0 \tLength: \t 10 \tOffset: \t 40 \n",
      "\n",
      "\tText: \t April 4, 1975 \tCategory: \t DateTime \tSubCategory: \t Date \n",
      "\tConfidence Score: \t 1.0 \tLength: \t 13 \tOffset: \t 54 \n",
      "\n",
      "\tText: \t develop \tCategory: \t Skill \tSubCategory: \t None \n",
      "\tConfidence Score: \t 0.98 \tLength: \t 7 \tOffset: \t 81 \n",
      "\n",
      "\tText: \t sell \tCategory: \t Skill \tSubCategory: \t None \n",
      "\tConfidence Score: \t 0.75 \tLength: \t 4 \tOffset: \t 93 \n",
      "\n",
      "\tText: \t Altair 8800 \tCategory: \t Product \tSubCategory: \t None \n",
      "\tConfidence Score: \t 0.99 \tLength: \t 11 \tOffset: \t 125 \n",
      "\n",
      "\tText: \t Microsoft \tCategory: \t Organization \tSubCategory: \t None \n",
      "\tConfidence Score: \t 0.96 \tLength: \t 9 \tOffset: \t 168 \n",
      "\n",
      "\tText: \t Gates \tCategory: \t Person \tSubCategory: \t None \n",
      "\tConfidence Score: \t 0.99 \tLength: \t 5 \tOffset: \t 179 \n",
      "\n",
      "\tText: \t chairman \tCategory: \t PersonType \tSubCategory: \t None \n",
      "\tConfidence Score: \t 0.87 \tLength: \t 8 \tOffset: \t 207 \n",
      "\n",
      "\tText: \t chief executive officer \tCategory: \t PersonType \tSubCategory: \t None \n",
      "\tConfidence Score: \t 0.99 \tLength: \t 23 \tOffset: \t 225 \n",
      "\n",
      "\tText: \t president \tCategory: \t PersonType \tSubCategory: \t None \n",
      "\tConfidence Score: \t 0.98 \tLength: \t 9 \tOffset: \t 250 \n",
      "\n",
      "\tText: \t chief software architect \tCategory: \t PersonType \tSubCategory: \t None \n",
      "\tConfidence Score: \t 0.97 \tLength: \t 24 \tOffset: \t 264 \n",
      "\n",
      "\tText: \t shareholder \tCategory: \t PersonType \tSubCategory: \t None \n",
      "\tConfidence Score: \t 0.75 \tLength: \t 11 \tOffset: \t 339 \n",
      "\n",
      "\tText: \t until May 2014. \tCategory: \t DateTime \tSubCategory: \t DateRange \n",
      "\tConfidence Score: \t 1.0 \tLength: \t 15 \tOffset: \t 351 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# This example requires environment variables named \"LANGUAGE_KEY\" and \"LANGUAGE_ENDPOINT\"\n",
    "language_key = ''\n",
    "language_endpoint = ''\n",
    "\n",
    "from azure.ai.textanalytics import TextAnalyticsClient\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "# Authenticate the client using your key and endpoint \n",
    "def authenticate_client():\n",
    "    ta_credential = AzureKeyCredential(language_key)\n",
    "    text_analytics_client = TextAnalyticsClient(\n",
    "            endpoint=language_endpoint, \n",
    "            credential=ta_credential)\n",
    "    return text_analytics_client\n",
    "\n",
    "client = authenticate_client()\n",
    "\n",
    "# Example function for recognizing entities from text\n",
    "def entity_recognition_example(client):\n",
    "\n",
    "    try:\n",
    "        documents = [\"\"\"Microsoft was founded by Bill Gates and Paul Allen on April 4, 1975, \n",
    "        to develop and sell BASIC interpreters for the Altair 8800. \n",
    "        During his career at Microsoft, Gates held the positions of chairman,\n",
    "        chief executive officer, president and chief software architect, \n",
    "        while also being the largest individual shareholder until May 2014.\"\"\"]\n",
    "        result = client.recognize_entities(documents = documents)[0]\n",
    "\n",
    "        print(\"Named Entities:\\n\")\n",
    "        for entity in result.entities:\n",
    "            print(\"\\tText: \\t\", entity.text, \"\\tCategory: \\t\", entity.category, \"\\tSubCategory: \\t\", entity.subcategory,\n",
    "                    \"\\n\\tConfidence Score: \\t\", round(entity.confidence_score, 2), \"\\tLength: \\t\", entity.length, \"\\tOffset: \\t\", entity.offset, \"\\n\")\n",
    "\n",
    "    except Exception as err:\n",
    "        print(\"Encountered exception. {}\".format(err))\n",
    "entity_recognition_example(client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linked Entities:\n",
      "\n",
      "\tName:  Microsoft \tId:  Microsoft \tUrl:  https://en.wikipedia.org/wiki/Microsoft \n",
      "\tData Source:  Wikipedia\n",
      "\tMatches:\n",
      "\t\tText: Microsoft\n",
      "\t\tConfidence Score: 0.55\n",
      "\t\tOffset: 0\n",
      "\t\tLength: 9\n",
      "\t\tText: Microsoft\n",
      "\t\tConfidence Score: 0.55\n",
      "\t\tOffset: 168\n",
      "\t\tLength: 9\n",
      "\tName:  Bill Gates \tId:  Bill Gates \tUrl:  https://en.wikipedia.org/wiki/Bill_Gates \n",
      "\tData Source:  Wikipedia\n",
      "\tMatches:\n",
      "\t\tText: Bill Gates\n",
      "\t\tConfidence Score: 0.63\n",
      "\t\tOffset: 25\n",
      "\t\tLength: 10\n",
      "\t\tText: Gates\n",
      "\t\tConfidence Score: 0.63\n",
      "\t\tOffset: 179\n",
      "\t\tLength: 5\n",
      "\tName:  Paul Allen \tId:  Paul Allen \tUrl:  https://en.wikipedia.org/wiki/Paul_Allen \n",
      "\tData Source:  Wikipedia\n",
      "\tMatches:\n",
      "\t\tText: Paul Allen\n",
      "\t\tConfidence Score: 0.60\n",
      "\t\tOffset: 40\n",
      "\t\tLength: 10\n",
      "\tName:  April 4 \tId:  April 4 \tUrl:  https://en.wikipedia.org/wiki/April_4 \n",
      "\tData Source:  Wikipedia\n",
      "\tMatches:\n",
      "\t\tText: April 4\n",
      "\t\tConfidence Score: 0.32\n",
      "\t\tOffset: 54\n",
      "\t\tLength: 7\n",
      "\tName:  BASIC \tId:  BASIC \tUrl:  https://en.wikipedia.org/wiki/BASIC \n",
      "\tData Source:  Wikipedia\n",
      "\tMatches:\n",
      "\t\tText: BASIC\n",
      "\t\tConfidence Score: 0.33\n",
      "\t\tOffset: 98\n",
      "\t\tLength: 5\n",
      "\tName:  Altair 8800 \tId:  Altair 8800 \tUrl:  https://en.wikipedia.org/wiki/Altair_8800 \n",
      "\tData Source:  Wikipedia\n",
      "\tMatches:\n",
      "\t\tText: Altair 8800\n",
      "\t\tConfidence Score: 0.88\n",
      "\t\tOffset: 125\n",
      "\t\tLength: 11\n"
     ]
    }
   ],
   "source": [
    "# This example requires environment variables named \"LANGUAGE_KEY\" and \"LANGUAGE_ENDPOINT\"\n",
    "language_key = ''\n",
    "language_endpoint = ''\n",
    "\n",
    "from azure.ai.textanalytics import TextAnalyticsClient\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "# Authenticate the client using your key and endpoint. \n",
    "def authenticate_client():\n",
    "    ta_credential = AzureKeyCredential(language_key)\n",
    "    text_analytics_client = TextAnalyticsClient(\n",
    "            endpoint=language_endpoint, \n",
    "            credential=ta_credential)\n",
    "    return text_analytics_client\n",
    "\n",
    "client = authenticate_client()\n",
    "\n",
    "# Example function for recognizing entities and providing a link to an online data source.\n",
    "def entity_linking_example(client):\n",
    "\n",
    "    try:\n",
    "        documents = [\"\"\"Microsoft was founded by Bill Gates and Paul Allen on April 4, 1975, \n",
    "        to develop and sell BASIC interpreters for the Altair 8800. \n",
    "        During his career at Microsoft, Gates held the positions of chairman,\n",
    "        chief executive officer, president and chief software architect, \n",
    "        while also being the largest individual shareholder until May 2014.\"\"\"]\n",
    "        result = client.recognize_linked_entities(documents = documents)[0]\n",
    "\n",
    "        print(\"Linked Entities:\\n\")\n",
    "        for entity in result.entities:\n",
    "            print(\"\\tName: \", entity.name, \"\\tId: \", entity.data_source_entity_id, \"\\tUrl: \", entity.url,\n",
    "            \"\\n\\tData Source: \", entity.data_source)\n",
    "            print(\"\\tMatches:\")\n",
    "            for match in entity.matches:\n",
    "                print(\"\\t\\tText:\", match.text)\n",
    "                print(\"\\t\\tConfidence Score: {0:.2f}\".format(match.confidence_score))\n",
    "                print(\"\\t\\tOffset: {}\".format(match.offset))\n",
    "                print(\"\\t\\tLength: {}\".format(match.length))\n",
    "            \n",
    "    except Exception as err:\n",
    "        print(\"Encountered exception. {}\".format(err))\n",
    "entity_linking_example(client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language:  English\n"
     ]
    }
   ],
   "source": [
    "# This example requires environment variables named \"LANGUAGE_KEY\" and \"LANGUAGE_ENDPOINT\"\n",
    "language_key = ''\n",
    "language_endpoint = ''\n",
    "\n",
    "from azure.ai.textanalytics import TextAnalyticsClient\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "# Authenticate the client using your key and endpoint \n",
    "def authenticate_client():\n",
    "    ta_credential = AzureKeyCredential(language_key)\n",
    "    text_analytics_client = TextAnalyticsClient(\n",
    "            endpoint=language_endpoint, \n",
    "            credential=ta_credential)\n",
    "    return text_analytics_client\n",
    "\n",
    "client = authenticate_client()\n",
    "\n",
    "# Example method for detecting the language of text\n",
    "def language_detection_example(client):\n",
    "    try:\n",
    "        # documents = [\"Ce document est rédigé en Français.\"]\n",
    "        documents = [\"안녕하세여 반가워요.Hi Hello\"]\n",
    "        response = client.detect_language(documents = documents, country_hint = 'us')[0]\n",
    "        print(\"Language: \", response.primary_language.name)\n",
    "\n",
    "    except Exception as err:\n",
    "        print(\"Encountered exception. {}\".format(err))\n",
    "language_detection_example(client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key phrases in article #1: 흉부 CT 스캔, AP 엑스레이가, 마테오 고메즈, 28세 남성, 할리우드 블러바드, 로스앤젤레스에, 콘토소 종합병원, 오후, 시, 환자, 갈비뼈 골절 가능성, 흉부 외상\n",
      "Key phrases in article #2: 결과, 흉부 대동맥, 가성 동맥류, 세 번째 갈비뼈, 환자는, 치료, 출혈, 스텐트\n",
      "Key phrases in article #3: 혈중 산소 수준, 치료가, 환자, 2022년 9월 1일, 보호자인, 니콜라우스 슐츠, 감독\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------------------------------------\n",
    "# Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "# Licensed under the MIT License. See License.txt in the project root for\n",
    "# license information.\n",
    "# --------------------------------------------------------------------------\n",
    "\n",
    "\"\"\"\n",
    "FILE: sample_extract_key_phrases.py\n",
    "\n",
    "DESCRIPTION:\n",
    "    This sample demonstrates how to extract key talking points from a batch of documents.\n",
    "\n",
    "    In this sample, we want to go over articles and read the ones that mention Microsoft.\n",
    "    We're going to use the SDK to create a rudimentary search algorithm to find these articles.\n",
    "\n",
    "USAGE:\n",
    "    python sample_extract_key_phrases.py\n",
    "\n",
    "    Set the environment variables with your own values before running the sample:\n",
    "    1) AZURE_LANGUAGE_ENDPOINT - the endpoint to your Language resource.\n",
    "    2) AZURE_LANGUAGE_KEY - your Language subscription key\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def sample_extract_key_phrases() -> None:\n",
    "    # print(\n",
    "    #     \"In this sample, we want to find the articles that mention Microsoft to read.\"\n",
    "    # )\n",
    "    # articles_that_mention_microsoft = []\n",
    "    # [START extract_key_phrases]\n",
    "    import os\n",
    "    from azure.core.credentials import AzureKeyCredential\n",
    "    from azure.ai.textanalytics import TextAnalyticsClient\n",
    "\n",
    "    endpoint = \"\"\n",
    "    key = \"\"\n",
    "\n",
    "    text_analytics_client = TextAnalyticsClient(endpoint=endpoint, credential=AzureKeyCredential(key))\n",
    "    articles = [\n",
    "        \"\"\"\n",
    "        마테오 고메즈(28세 남성)는 2022년 8월 17일 할리우드 블러바드 인근에서 자동차 사고를 당해 로스앤젤레스에 있는 콘토소 종합병원에 오후 7시 45분에 입원했습니다. \n",
    "        환자는 갈비뼈 골절 가능성을 나타내는 흉부 외상을 보였고 호흡 곤란이 있었습니다. 갈비뼈와 폐의 손상을 확인하기 위해 흉부 CT 스캔과 AP 엑스레이가 시행되었습니다. \n",
    "        \n",
    "        \"\"\",\n",
    "        \"\"\"\n",
    "        결과는 흉부 대동맥의 가성 동맥류와 오른쪽 첫 번째 및 세 번째 갈비뼈의 경미한 골절을 보여주었습니다. \n",
    "        환자는 치료가 시작된 후 중환자실(ICU)에 머물렀고, 출혈을 안정화하기 위해 스텐트가 수술적으로 삽입되었습니다. \n",
    "        \"\"\",\n",
    "        \"\"\"\n",
    "        혈중 산소 수준이 95%에 도달할 때까지 치료가 진행되었습니다. 환자는 2022년 9월 1일, 보호자인 니콜라우스 슐츠의 감독 하에 퇴원했습니다.\n",
    "        \"\"\"\n",
    "    ]\n",
    "\n",
    "    result = text_analytics_client.extract_key_phrases(articles, language = 'ko')\n",
    "    for idx, doc in enumerate(result):\n",
    "        if not doc.is_error:\n",
    "            print(\"Key phrases in article #{}: {}\".format(\n",
    "                idx + 1,\n",
    "                \", \".join(doc.key_phrases)\n",
    "            ))\n",
    "    # # [END extract_key_phrases]\n",
    "    #         if \"Microsoft\" in doc.key_phrases:\n",
    "    #             articles_that_mention_microsoft.append(str(idx + 1))\n",
    "\n",
    "    # print(\n",
    "    #     \"The articles that mention Microsoft are articles number: {}. Those are the ones I'm interested in reading.\".format(\n",
    "    #         \", \".join(articles_that_mention_microsoft)\n",
    "    #     )\n",
    "    # )\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    sample_extract_key_phrases()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document Sentiment: neutral\n",
      "Overall scores: positive=0.29; neutral=0.67; negative=0.04 \n",
      "\n",
      "Sentence: 혹시 대기시간이 길어질까봐 미리 전화로 문의했으며 전화통화시 검사 후 검사 결과가 바로 나온다고 해서 믿고 방문하였음.\n",
      "Sentence sentiment: neutral\n",
      "Sentence score:\n",
      "Positive=0.29\n",
      "Neutral=0.67\n",
      "Negative=0.04\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Document Sentiment: negative\n",
      "Overall scores: positive=0.00; neutral=0.00; negative=1.00 \n",
      "\n",
      "Sentence: 그러나 간호사는 매우 불친절했으며 검사 결과가 나오는데는 2~3일이 걸린다고 의사가 말함.\n",
      "Sentence sentiment: negative\n",
      "Sentence score:\n",
      "Positive=0.00\n",
      "Neutral=0.00\n",
      "Negative=1.00\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Document Sentiment: negative\n",
      "Overall scores: positive=0.01; neutral=0.02; negative=0.97 \n",
      "\n",
      "Sentence: 다시는 이 병원을 이용하지 않겠음.절대 가지 말것!!\n",
      "Sentence sentiment: negative\n",
      "Sentence score:\n",
      "Positive=0.01\n",
      "Neutral=0.02\n",
      "Negative=0.97\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Document Sentiment: positive\n",
      "Overall scores: positive=0.84; neutral=0.16; negative=0.00 \n",
      "\n",
      "Sentence: 병원은 매우 깨끗했음.\n",
      "Sentence sentiment: positive\n",
      "Sentence score:\n",
      "Positive=0.84\n",
      "Neutral=0.16\n",
      "Negative=0.00\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# This example requires environment variables named \"LANGUAGE_KEY\" and \"LANGUAGE_ENDPOINT\"\n",
    "language_key = ''\n",
    "language_endpoint = ''\n",
    "\n",
    "from azure.ai.textanalytics import TextAnalyticsClient\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "# Authenticate the client using your key and endpoint \n",
    "def authenticate_client():\n",
    "    ta_credential = AzureKeyCredential(language_key)\n",
    "    text_analytics_client = TextAnalyticsClient(\n",
    "            endpoint=language_endpoint, \n",
    "            credential=ta_credential)\n",
    "    return text_analytics_client\n",
    "\n",
    "client = authenticate_client()\n",
    "\n",
    "# Example method for detecting sentiment and opinions in text \n",
    "def sentiment_analysis_with_opinion_mining_example(client):\n",
    "\n",
    "    documents = [\n",
    "        \"혹시 대기시간이 길어질까봐 미리 전화로 문의했으며 전화통화시 검사 후 검사 결과가 바로 나온다고 해서 믿고 방문하였음.\",\n",
    "        \"그러나 간호사는 매우 불친절했으며 검사 결과가 나오는데는 2~3일이 걸린다고 의사가 말함.\",\n",
    "        \"다시는 이 병원을 이용하지 않겠음.\"\n",
    "        \"절대 가지 말것!!\",\n",
    "        \"병원은 매우 깨끗했음.\"\n",
    "    ]\n",
    "\n",
    "    result = client.analyze_sentiment(documents, show_opinion_mining=True, language='ko')\n",
    "    doc_result = [doc for doc in result if not doc.is_error]\n",
    "\n",
    "    positive_reviews = [doc for doc in doc_result if doc.sentiment == \"positive\"]\n",
    "    negative_reviews = [doc for doc in doc_result if doc.sentiment == \"negative\"]\n",
    "\n",
    "    positive_mined_opinions = []\n",
    "    mixed_mined_opinions = []\n",
    "    negative_mined_opinions = []\n",
    "\n",
    "    for document in doc_result:\n",
    "        print(\"Document Sentiment: {}\".format(document.sentiment))\n",
    "        print(\"Overall scores: positive={0:.2f}; neutral={1:.2f}; negative={2:.2f} \\n\".format(\n",
    "            document.confidence_scores.positive,\n",
    "            document.confidence_scores.neutral,\n",
    "            document.confidence_scores.negative,\n",
    "        ))\n",
    "        for sentence in document.sentences:\n",
    "            print(\"Sentence: {}\".format(sentence.text))\n",
    "            print(\"Sentence sentiment: {}\".format(sentence.sentiment))\n",
    "            print(\"Sentence score:\\nPositive={0:.2f}\\nNeutral={1:.2f}\\nNegative={2:.2f}\\n\".format(\n",
    "                sentence.confidence_scores.positive,\n",
    "                sentence.confidence_scores.neutral,\n",
    "                sentence.confidence_scores.negative,\n",
    "            ))\n",
    "            for mined_opinion in sentence.mined_opinions:\n",
    "                target = mined_opinion.target\n",
    "                print(\"......'{}' target '{}'\".format(target.sentiment, target.text))\n",
    "                print(\"......Target score:\\n......Positive={0:.2f}\\n......Negative={1:.2f}\\n\".format(\n",
    "                    target.confidence_scores.positive,\n",
    "                    target.confidence_scores.negative,\n",
    "                ))\n",
    "                for assessment in mined_opinion.assessments:\n",
    "                    print(\"......'{}' assessment '{}'\".format(assessment.sentiment, assessment.text))\n",
    "                    print(\"......Assessment score:\\n......Positive={0:.2f}\\n......Negative={1:.2f}\\n\".format(\n",
    "                        assessment.confidence_scores.positive,\n",
    "                        assessment.confidence_scores.negative,\n",
    "                    ))\n",
    "            print(\"\\n\")\n",
    "        print(\"\\n\")\n",
    "          \n",
    "sentiment_analysis_with_opinion_mining_example(client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting azure-ai-textanalytics==5.3.0Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "  Downloading azure_ai_textanalytics-5.3.0-py3-none-any.whl.metadata (82 kB)\n",
      "Requirement already satisfied: azure-core<2.0.0,>=1.24.0 in c:\\users\\user\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from azure-ai-textanalytics==5.3.0) (1.30.2)\n",
      "Requirement already satisfied: azure-common~=1.1 in c:\\users\\user\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from azure-ai-textanalytics==5.3.0) (1.1.28)\n",
      "Requirement already satisfied: isodate<1.0.0,>=0.6.1 in c:\\users\\user\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from azure-ai-textanalytics==5.3.0) (0.6.1)\n",
      "Requirement already satisfied: typing-extensions>=4.0.1 in c:\\users\\user\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from azure-ai-textanalytics==5.3.0) (4.12.2)\n",
      "Requirement already satisfied: requests>=2.21.0 in c:\\users\\user\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from azure-core<2.0.0,>=1.24.0->azure-ai-textanalytics==5.3.0) (2.32.3)\n",
      "Requirement already satisfied: six>=1.11.0 in c:\\users\\user\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from azure-core<2.0.0,>=1.24.0->azure-ai-textanalytics==5.3.0) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\user\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests>=2.21.0->azure-core<2.0.0,>=1.24.0->azure-ai-textanalytics==5.3.0) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\user\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests>=2.21.0->azure-core<2.0.0,>=1.24.0->azure-ai-textanalytics==5.3.0) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\user\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests>=2.21.0->azure-core<2.0.0,>=1.24.0->azure-ai-textanalytics==5.3.0) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests>=2.21.0->azure-core<2.0.0,>=1.24.0->azure-ai-textanalytics==5.3.0) (2024.7.4)\n",
      "Downloading azure_ai_textanalytics-5.3.0-py3-none-any.whl (298 kB)\n",
      "Installing collected packages: azure-ai-textanalytics\n",
      "  Attempting uninstall: azure-ai-textanalytics\n",
      "    Found existing installation: azure-ai-textanalytics 5.2.0\n",
      "    Uninstalling azure-ai-textanalytics-5.2.0:\n",
      "      Successfully uninstalled azure-ai-textanalytics-5.2.0\n",
      "Successfully installed azure-ai-textanalytics-5.3.0\n"
     ]
    }
   ],
   "source": [
    "pip install azure-ai-textanalytics==5.3.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: azure-ai-textanalytics in c:\\users\\user\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (5.3.0)\n",
      "Requirement already satisfied: azure-core<2.0.0,>=1.24.0 in c:\\users\\user\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from azure-ai-textanalytics) (1.30.2)\n",
      "Requirement already satisfied: azure-common~=1.1 in c:\\users\\user\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from azure-ai-textanalytics) (1.1.28)\n",
      "Requirement already satisfied: isodate<1.0.0,>=0.6.1 in c:\\users\\user\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from azure-ai-textanalytics) (0.6.1)\n",
      "Requirement already satisfied: typing-extensions>=4.0.1 in c:\\users\\user\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from azure-ai-textanalytics) (4.12.2)\n",
      "Requirement already satisfied: requests>=2.21.0 in c:\\users\\user\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from azure-core<2.0.0,>=1.24.0->azure-ai-textanalytics) (2.32.3)\n",
      "Requirement already satisfied: six>=1.11.0 in c:\\users\\user\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from azure-core<2.0.0,>=1.24.0->azure-ai-textanalytics) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\user\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests>=2.21.0->azure-core<2.0.0,>=1.24.0->azure-ai-textanalytics) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\user\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests>=2.21.0->azure-core<2.0.0,>=1.24.0->azure-ai-textanalytics) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\user\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests>=2.21.0->azure-core<2.0.0,>=1.24.0->azure-ai-textanalytics) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests>=2.21.0->azure-core<2.0.0,>=1.24.0->azure-ai-textanalytics) (2024.7.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade azure-ai-textanalytics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary extracted: \n",
      "추출적 요약 기능은 자연어 처리 기술을 사용하여 비구조적 텍스트 문서에서 핵심 문장을 찾아냅니다.\n"
     ]
    }
   ],
   "source": [
    "# This example requires environment variables named \"LANGUAGE_KEY\" and \"LANGUAGE_ENDPOINT\"\n",
    "key = ''\n",
    "endpoint = ''\n",
    "\n",
    "from azure.ai.textanalytics import TextAnalyticsClient\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "# Authenticate the client using your key and endpoint \n",
    "def authenticate_client():\n",
    "    ta_credential = AzureKeyCredential(key)\n",
    "    text_analytics_client = TextAnalyticsClient(\n",
    "            endpoint=endpoint, \n",
    "            credential=ta_credential)\n",
    "    return text_analytics_client\n",
    "\n",
    "client = authenticate_client()\n",
    "\n",
    "# Example method for summarizing text\n",
    "def sample_extractive_summarization(client):\n",
    "    from azure.core.credentials import AzureKeyCredential\n",
    "    from azure.ai.textanalytics import (\n",
    "        TextAnalyticsClient,\n",
    "        ExtractiveSummaryAction\n",
    "    ) \n",
    "\n",
    "    document = [\n",
    "        \"추출적 요약 기능은 자연어 처리 기술을 사용하여 비구조적 텍스트 문서에서 핵심 문장을 찾아냅니다. 이 문장들은 collectively 문서의 주요 아이디어를 전달합니다. 이 기능은 개발자를 위한 API로 제공됩니다. 개발자들은 이를 사용하여 추출된 관련 정보를 기반으로 다양한 사용 사례를 지원하는 지능형 솔루션을 구축할 수 있습니다. 추출적 요약은 여러 언어를 지원합니다. 이 기능은 사전 훈련된 다국어 변환기 모델을 기반으로 하며, 전체적인 표현을 추구하는 일환으로 개발되었습니다. 단일 언어 간의 전이 학습에서 강점을 끌어내어 언어의 공통된 특성을 활용하여 품질과 효율성이 향상된 모델을 생성합니다.\"\n",
    "    ]\n",
    "\n",
    "    poller = client.begin_analyze_actions(\n",
    "        document,\n",
    "        actions=[\n",
    "            ExtractiveSummaryAction(max_sentence_count=1)\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    document_results = poller.result()\n",
    "    for result in document_results:\n",
    "        extract_summary_result = result[0]  # first document, first result\n",
    "        if extract_summary_result.is_error:\n",
    "            print(\"...Is an error with code '{}' and message '{}'\".format(\n",
    "                extract_summary_result.code, extract_summary_result.message\n",
    "            ))\n",
    "        else:\n",
    "            print(\"Summary extracted: \\n{}\".format(\n",
    "                \" \".join([sentence.text for sentence in extract_summary_result.sentences]))\n",
    "            )\n",
    "\n",
    "sample_extractive_summarization(client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summaries abstracted:\n",
      "추출적요약기는 자연어 처리 기술을 통해 텍스트 데이터에서 중요한 문장을 찾아내는 기능입니다. 이 기능은 주요 아이디어를 전달하여 복잡한 정보를 풀 수 있습니다. 개발자들은 이를 사용하여 다양한 사용 사례를 제공하여 솔루션을 구축할 수 있습니다. 이 기능은 여러 언어를 지원하여 다국어 변환을 통해 일환되어 언어의 공통 특성을 활용한 모델을 얻습니다.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------------------------------------\n",
    "# Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "# Licensed under the MIT License. See License.txt in the project root for\n",
    "# license information.\n",
    "# --------------------------------------------------------------------------\n",
    "\n",
    "\"\"\"\n",
    "FILE: sample_abstract_summary.py\n",
    "\n",
    "DESCRIPTION:\n",
    "    This sample demonstrates how to submit text documents for abstractive text summarization.\n",
    "    Abstractive summarization is available as an action type through the begin_analyze_actions API.\n",
    "\n",
    "    Abstractive summarization generates a summary that may not use the same words as those in\n",
    "    the document, but captures the main idea.\n",
    "\n",
    "USAGE:\n",
    "    python sample_abstract_summary.py\n",
    "\n",
    "    Set the environment variables with your own values before running the sample:\n",
    "    1) AZURE_LANGUAGE_ENDPOINT - the endpoint to your Language resource.\n",
    "    2) AZURE_LANGUAGE_KEY - your Language subscription key\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def sample_abstractive_summarization() -> None:\n",
    "    # [START abstract_summary]\n",
    "    import os\n",
    "    from azure.core.credentials import AzureKeyCredential\n",
    "    from azure.ai.textanalytics import TextAnalyticsClient\n",
    "\n",
    "    key = '7ec744efc6af4fdd946c310606aac1bc'\n",
    "    endpoint = 'https://st023-lang.cognitiveservices.azure.com/'\n",
    "\n",
    "    text_analytics_client = TextAnalyticsClient(\n",
    "        endpoint=endpoint,\n",
    "        credential=AzureKeyCredential(key),\n",
    "    )\n",
    "\n",
    "    document = [\n",
    "        \"추출적 요약 기능은 자연어 처리 기술을 사용하여 비구조적 텍스트 문서에서 핵심 문장을 찾아냅니다. 이 문장들은 collectively 문서의 주요 아이디어를 전달합니다. 이 기능은 개발자를 위한 API로 제공됩니다. 개발자들은 이를 사용하여 추출된 관련 정보를 기반으로 다양한 사용 사례를 지원하는 지능형 솔루션을 구축할 수 있습니다. 추출적 요약은 여러 언어를 지원합니다. 이 기능은 사전 훈련된 다국어 변환기 모델을 기반으로 하며, 전체적인 표현을 추구하는 일환으로 개발되었습니다. 단일 언어 간의 전이 학습에서 강점을 끌어내어 언어의 공통된 특성을 활용하여 품질과 효율성이 향상된 모델을 생성합니다.\"\n",
    "    ]\n",
    "\n",
    "    poller = text_analytics_client.begin_abstract_summary(document)\n",
    "    abstract_summary_results = poller.result()\n",
    "    for result in abstract_summary_results:\n",
    "        if result.kind == \"AbstractiveSummarization\":\n",
    "            print(\"Summaries abstracted:\")\n",
    "            [print(f\"{summary.text}\\n\") for summary in result.summaries]\n",
    "        elif result.is_error is True:\n",
    "            print(\"...Is an error with code '{}' and message '{}'\".format(\n",
    "                result.error.code, result.error.message\n",
    "            ))\n",
    "    # [END abstract_summary]\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    sample_abstractive_summarization()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
